master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
| distributed init (rank 0, world 1): env://
2026-01-17 14:34:53,976 [INFO] 
=====  Running Parameters    =====
2026-01-17 14:34:53,976 [INFO] {
    "amp": true,
    "batch_size_eval": 4,
    "batch_size_train": 4,
    "device": "cuda",
    "dist_backend": "nccl",
    "dist_url": "env://",
    "distributed": true,
    "evaluate": false,
    "gpu": 0,
    "init_lr": 0.0001,
    "lr_sched": "linear_warmup_cosine_lr",
    "max_epoch": 15,
    "min_lr": 1e-05,
    "num_workers": 24,
    "output_dir": "output/drivegpt/cvpr/",
    "rank": 0,
    "resume_ckpt_path": null,
    "seed": 42,
    "task": "carla_drive",
    "train_splits": [
        "train"
    ],
    "valid_splits": [
        "val"
    ],
    "warmup_lr": 1e-06,
    "warmup_steps": 2000,
    "weight_decay": 0.06,
    "world_size": 1
}
2026-01-17 14:34:53,976 [INFO] 
======  Dataset Attributes  ======
2026-01-17 14:34:53,976 [INFO] 
======== carla_voice =======
2026-01-17 14:34:53,977 [INFO] {
    "build_info": {
        "annotations": {
            "train": {
                "enable_notice": true,
                "enable_start_frame_augment": true,
                "scale": [
                    0.95,
                    1.05
                ],
                "storage": "/scratch/project_2014099/data-lmdrive/data/Town10",
                "token_max_length": 40,
                "towns": [
                    1,
                    2,
                    3,
                    4,
                    5,
                    6,
                    7,
                    10
                ],
                "weathers": [
                    0,
                    1,
                    2,
                    3,
                    4,
                    5,
                    6,
                    7,
                    8,
                    9,
                    10,
                    11,
                    14,
                    15,
                    16,
                    17,
                    18,
                    19
                ]
            },
            "val": {
                "enable_notice": true,
                "enable_start_frame_augment": true,
                "scale": [
                    0.95,
                    1.05
                ],
                "storage": "/scratch/project_2014099/data-lmdrive/data/Town10",
                "token_max_length": 40,
                "towns": [
                    1,
                    2,
                    3,
                    4,
                    5,
                    6,
                    7,
                    10
                ],
                "weathers": [
                    12,
                    13,
                    20
                ]
            }
        }
    }
}
2026-01-17 14:34:53,977 [INFO] 
======  Model Attributes  ======
2026-01-17 14:34:53,978 [INFO] {
    "arch": "vicuna_drive",
    "drop_path_rate": 0,
    "finetuned": "",
    "freeze_vit": true,
    "image_size": 224,
    "llm_model": "bczhou/TinyLLaVA-2.0B",
    "load_finetuned": false,
    "load_pretrained": true,
    "max_txt_len": 64,
    "model_type": "vicuna7b",
    "num_query_token": 32,
    "preception_model": "memfuser_baseline_e1d3_return_feature",
    "preception_model_ckpt": "../vision_encoder/sensor_pretrain.pth.tar.r50",
    "pretrained": "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/InstructBLIP/instruct_blip_vicuna7b_trimmed.pth",
    "prompt": "",
    "split_section_num_for_visual_encoder": 2,
    "use_extra_prompt": false,
    "use_grad_checkpoint": false,
    "use_notice_prompt": true,
    "vit_precision": "fp16"
}
2026-01-17 14:34:54,699 [INFO] Scenario nums: 15159
2026-01-17 14:34:55,220 [INFO] Scenario nums: 2822
/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
2026-01-17 14:34:55,856 [INFO] Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet50d_ra2-464e36ba.pth)
/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/torch/nn/modules/conv.py:459: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)
  return F.conv2d(input, weight, bias, self.stride,
2026-01-17 14:34:59,646 [INFO] freeze vision encoder
2026-01-17 14:34:59,891 [INFO] Lock 140721413210752 acquired on /scratch/project_2014099/hf-hub-cache/.locks/models--bczhou--TinyLLaVA-3.1B/5545fb7cf0935d5c48abc8c28e7475f46c0046e7.lock
2026-01-17 14:35:00,134 [INFO] Lock 140721413210752 released on /scratch/project_2014099/hf-hub-cache/.locks/models--bczhou--TinyLLaVA-3.1B/5545fb7cf0935d5c48abc8c28e7475f46c0046e7.lock
2026-01-17 14:35:00,435 [INFO] Lock 140733600588416 acquired on /scratch/project_2014099/hf-hub-cache/.locks/models--bczhou--TinyLLaVA-3.1B/84ef7fb594b5c0979e48bdeddb60a0adef33df0b.lock
2026-01-17 14:35:00,810 [INFO] Lock 140733600588416 released on /scratch/project_2014099/hf-hub-cache/.locks/models--bczhou--TinyLLaVA-3.1B/84ef7fb594b5c0979e48bdeddb60a0adef33df0b.lock
2026-01-17 14:35:01,289 [INFO] Lock 140721413210752 acquired on /scratch/project_2014099/hf-hub-cache/.locks/models--bczhou--TinyLLaVA-3.1B/226b0752cac7789c48f0cb3ec53eda48b7be36cc.lock
2026-01-17 14:35:01,622 [INFO] Lock 140721413210752 released on /scratch/project_2014099/hf-hub-cache/.locks/models--bczhou--TinyLLaVA-3.1B/226b0752cac7789c48f0cb3ec53eda48b7be36cc.lock
2026-01-17 14:35:01,991 [INFO] Lock 140733600588416 acquired on /scratch/project_2014099/hf-hub-cache/.locks/models--bczhou--TinyLLaVA-3.1B/7debb4784a7d53328d4d021fc46314bec4af3833.lock
2026-01-17 14:35:02,175 [INFO] Lock 140733600588416 released on /scratch/project_2014099/hf-hub-cache/.locks/models--bczhou--TinyLLaVA-3.1B/7debb4784a7d53328d4d021fc46314bec4af3833.lock
2026-01-17 14:35:02,520 [INFO] Lock 140718924988960 acquired on /scratch/project_2014099/hf-hub-cache/.locks/models--bczhou--TinyLLaVA-3.1B/330140f0678dc92ae683e1c1cccffc6a001251ae.lock
2026-01-17 14:35:02,691 [INFO] Lock 140718924988960 released on /scratch/project_2014099/hf-hub-cache/.locks/models--bczhou--TinyLLaVA-3.1B/330140f0678dc92ae683e1c1cccffc6a001251ae.lock
2026-01-17 14:35:03,066 [INFO] Lock 140718924920720 acquired on /scratch/project_2014099/hf-hub-cache/.locks/models--bczhou--TinyLLaVA-2.0B/760f5f0a19b3910b646172053239bc407820d500.lock
2026-01-17 14:35:03,247 [INFO] Lock 140718924920720 released on /scratch/project_2014099/hf-hub-cache/.locks/models--bczhou--TinyLLaVA-2.0B/760f5f0a19b3910b646172053239bc407820d500.lock
2026-01-17 14:35:03,410 [INFO] Lock 140718924989728 acquired on /scratch/project_2014099/hf-hub-cache/.locks/models--bczhou--TinyLLaVA-2.0B/6ea6d36d10e10fdae9b90f56a961fdde5cb8795ebc26408f5351877d129d18dd.lock
2026-01-17 14:36:17,258 [INFO] Lock 140718924989728 released on /scratch/project_2014099/hf-hub-cache/.locks/models--bczhou--TinyLLaVA-2.0B/6ea6d36d10e10fdae9b90f56a961fdde5cb8795ebc26408f5351877d129d18dd.lock
2026-01-17 14:36:18,683 [INFO] Lock 140718924992128 acquired on /scratch/project_2014099/hf-hub-cache/.locks/models--bczhou--TinyLLaVA-2.0B/1b42e2d72b64787cba95afba2bae6aee917af01a.lock
2026-01-17 14:36:18,885 [INFO] Lock 140718924992128 released on /scratch/project_2014099/hf-hub-cache/.locks/models--bczhou--TinyLLaVA-2.0B/1b42e2d72b64787cba95afba2bae6aee917af01a.lock
Loading Q-Former
2026-01-17 14:36:31,335 [INFO] Lock 140718924922016 acquired on /scratch/project_2014099/hf-hub-cache/.locks/models--bert-base-uncased/68d45e234eb4a928074dfd868cead0219ab85354cc53d20e772753c6bb9169d3.lock
2026-01-17 14:36:33,316 [INFO] Lock 140718924922016 released on /scratch/project_2014099/hf-hub-cache/.locks/models--bert-base-uncased/68d45e234eb4a928074dfd868cead0219ab85354cc53d20e772753c6bb9169d3.lock
2026-01-17 14:36:35,551 [INFO] Start training
2026-01-17 14:36:36,042 [INFO] dataset_ratios not specified, datasets will be concatenated (map-style datasets) or chained (webdataset.DataPipeline).
15159
2822
2026-01-17 14:36:36,042 [INFO] Loaded 15159 records for train split from the dataset.
2026-01-17 14:36:36,042 [INFO] Loaded 2822 records for val split from the dataset.
/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 24 worker processes in total. Our suggested max number of worker in current system is 10, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
trainable parameter shape:  torch.Size([1, 4, 768])
trainable parameter shape:  torch.Size([2048, 2048])
trainable parameter shape:  torch.Size([10, 2048])
trainable parameter shape:  torch.Size([2048, 2048])
trainable parameter shape:  torch.Size([2, 2048])
trainable parameter shape:  torch.Size([50297, 768])
trainable parameter shape:  torch.Size([512, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([2048, 768])
trainable parameter shape:  torch.Size([256])
trainable parameter shape:  torch.Size([256])
trainable parameter shape:  torch.Size([2048])
trainable parameter shape:  torch.Size([10])
trainable parameter shape:  torch.Size([2048])
trainable parameter shape:  torch.Size([2])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([2048])
2026-01-17 14:36:36,049 [INFO] number of trainable parameters: 200225548
2026-01-17 14:36:36,050 [INFO] Start training epoch 0, 3789 iters per inner epoch.
2026-01-17 14:36:37,850 [INFO] /scratch/project_2014099/data-lmdrive/data/Town10/routes_town10_tiny_w1_09_10_16_31_03/rgb_full/0047.jpg
2026-01-17 14:36:37,853 [INFO] /scratch/project_2014099/data-lmdrive/data/Town10/routes_town10_tiny_w10_09_09_20_38_08/rgb_full/0016.jpg
2026-01-17 14:36:37,867 [INFO] /scratch/project_2014099/data-lmdrive/data/Town10/routes_town10_tiny_w14_09_10_20_22_39/rgb_full/0083.jpg
2026-01-17 14:36:37,868 [INFO] /scratch/project_2014099/data-lmdrive/data/Town10/routes_town10_tiny_w3_09_10_12_08_24/rgb_full/0176.jpg
2026-01-17 14:36:37,869 [INFO] /scratch/project_2014099/data-lmdrive/data/Town10/routes_town10_tiny_w11_09_10_18_05_39/rgb_full/0016.jpg
2026-01-17 14:36:37,880 [INFO] /scratch/project_2014099/data-lmdrive/data/Town10/routes_town10_tiny_w1_09_09_20_43_10/rgb_full/0134.jpg
2026-01-17 14:36:37,880 [INFO] /scratch/project_2014099/data-lmdrive/data/Town10/routes_town10_tiny_w10_09_09_21_51_01/rgb_full/0016.jpg
2026-01-17 14:36:37,882 [INFO] /scratch/project_2014099/data-lmdrive/data/Town10/routes_town10_tiny_w10_09_10_08_32_07/rgb_full/0086.jpg
2026-01-17 14:36:37,891 [INFO] /scratch/project_2014099/data-lmdrive/data/Town10/routes_town10_tiny_w5_09_10_03_26_53/rgb_full/0000.jpg
2026-01-17 14:36:37,894 [INFO] /scratch/project_2014099/data-lmdrive/data/Town10/routes_town10_tiny_w7_09_10_18_19_36/rgb_full/0192.jpg
2026-01-17 14:36:37,894 [INFO] /scratch/project_2014099/data-lmdrive/data/Town10/routes_town10_tiny_w7_09_10_11_14_22/rgb_full/0057.jpg
2026-01-17 14:36:37,895 [INFO] /scratch/project_2014099/data-lmdrive/data/Town10/routes_town10_tiny_w15_09_10_19_01_03/rgb_full/0373.jpg
2026-01-17 14:36:37,895 [INFO] /scratch/project_2014099/data-lmdrive/data/Town10/routes_town10_tiny_w17_09_10_19_25_18/rgb_full/0092.jpg
2026-01-17 14:36:37,896 [INFO] /scratch/project_2014099/data-lmdrive/data/Town10/routes_town10_tiny_w17_09_09_19_35_23/rgb_full/0846.jpg
2026-01-17 14:36:37,900 [INFO] /scratch/project_2014099/data-lmdrive/data/Town10/routes_town10_tiny_w18_09_10_05_27_11/rgb_full/0547.jpg
2026-01-17 14:36:37,908 [INFO] /scratch/project_2014099/data-lmdrive/data/Town10/routes_town10_tiny_w2_09_10_08_57_58/rgb_full/0146.jpg
Traceback (most recent call last):
  File "/projappl/project_2014099/lmdrive-original/LAVIS/train.py", line 103, in <module>
    main()
  File "/projappl/project_2014099/lmdrive-original/LAVIS/train.py", line 99, in main
    runner.train()
  File "/projappl/project_2014099/lmdrive-original/LAVIS/lavis/runners/runner_base.py", line 378, in train
2026-01-17 14:36:37,920 [INFO] /scratch/project_2014099/data-lmdrive/data/Town10/routes_town10_tiny_w6_09_10_13_37_54/rgb_full/0015.jpg
2026-01-17 14:36:37,941 [INFO] /scratch/project_2014099/data-lmdrive/data/Town10/routes_town10_tiny_w1_09_10_12_55_04/rgb_full/0128.jpg
    train_stats = self.train_epoch(cur_epoch)
  File "/projappl/project_2014099/lmdrive-original/LAVIS/lavis/runners/runner_base.py", line 437, in train_epoch
    return self.task.train_epoch(
  File "/projappl/project_2014099/lmdrive-original/LAVIS/lavis/tasks/drive.py", line 143, in train_epoch
2026-01-17 14:36:37,947 [INFO] /scratch/project_2014099/data-lmdrive/data/Town10/routes_town10_tiny_w3_09_10_01_25_50/rgb_full/0366.jpg
2026-01-17 14:36:37,959 [INFO] /scratch/project_2014099/data-lmdrive/data/Town10/routes_town10_tiny_w1_09_10_11_55_54/rgb_full/0579.jpg
2026-01-17 14:36:37,960 [INFO] /scratch/project_2014099/data-lmdrive/data/Town10/routes_town10_tiny_w3_09_10_12_57_08/rgb_full/0017.jpg
2026-01-17 14:36:37,960 [INFO] /scratch/project_2014099/data-lmdrive/data/Town10/routes_town10_tiny_w2_09_10_05_06_57/rgb_full/0584.jpg
2026-01-17 14:36:37,960 [INFO] /scratch/project_2014099/data-lmdrive/data/Town10/routes_town10_tiny_w4_09_10_09_50_24/rgb_full/0436.jpg
    return self._train_inner_loop(
  File "/projappl/project_2014099/lmdrive-original/LAVIS/lavis/tasks/drive.py", line 236, in _train_inner_loop
    samples = next(data_loader)
  File "/projappl/project_2014099/lmdrive-original/LAVIS/lavis/datasets/datasets/dataloader_utils.py", line 147, in __next__
2026-01-17 14:36:37,969 [INFO] /scratch/project_2014099/data-lmdrive/data/Town10/routes_town10_tiny_w17_09_10_20_29_09/rgb_full/0015.jpg
2026-01-17 14:36:37,969 [INFO] /scratch/project_2014099/data-lmdrive/data/Town10/routes_town10_tiny_w5_09_10_02_28_01/rgb_full/0417.jpg
2026-01-17 14:36:37,970 [INFO] /scratch/project_2014099/data-lmdrive/data/Town10/routes_town10_tiny_w18_09_10_05_23_27/rgb_full/0181.jpg
    data = next(self.iter_loader)
  File "/projappl/project_2014099/lmdrive-original/LAVIS/lavis/datasets/datasets/dataloader_utils.py", line 60, in __iter__
2026-01-17 14:36:37,982 [INFO] /scratch/project_2014099/data-lmdrive/data/Town10/routes_town10_tiny_w0_09_10_16_56_35/rgb_full/0015.jpg
    self.preload(loader_it)
  File "/projappl/project_2014099/lmdrive-original/LAVIS/lavis/datasets/datasets/dataloader_utils.py", line 78, in preload
    self.batch = next(it)
  File "/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 633, in __next__
2026-01-17 14:36:37,982 [INFO] /scratch/project_2014099/data-lmdrive/data/Town10/routes_town10_tiny_w7_09_10_04_28_42/rgb_full/0139.jpg
    data = self._next_data()
  File "/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1345, in _next_data
    return self._process_data(data)
  File "/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1371, in _process_data
    data.reraise()
  File "/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/torch/_utils.py", line 644, in reraise
    raise exception
FileNotFoundError: Caught FileNotFoundError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/projappl/project_2014099/lmdrive-original/LAVIS/lavis/datasets/datasets/base_io_dataset.py", line 29, in _load_image
    img = Image.open(self.root_path + path)
  File "/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/PIL/Image.py", line 3431, in open
    fp = builtins.open(filename, "rb")
FileNotFoundError: [Errno 2] No such file or directory: '/scratch/project_2014099/data-lmdrive/data/Town10/routes_town10_tiny_w2_09_10_08_57_58/rgb_full/0146.jpg'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
    data = fetcher.fetch(index)
  File "/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/torch/utils/data/dataset.py", line 243, in __getitem__
    return self.datasets[dataset_idx][sample_idx]
  File "/projappl/project_2014099/lmdrive-original/LAVIS/lavis/datasets/datasets/carla_dataset_llm.py", line 308, in __getitem__
    sensor_data = self._extract_data_item(route_path, frame_id)
  File "/projappl/project_2014099/lmdrive-original/LAVIS/lavis/datasets/datasets/carla_dataset_llm.py", line 359, in _extract_data_item
    rgb_full_image = self._load_image(
  File "/projappl/project_2014099/lmdrive-original/LAVIS/lavis/datasets/datasets/base_io_dataset.py", line 34, in _load_image
    img = Image.open(self.root_path + new_path)
  File "/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/PIL/Image.py", line 3431, in open
    fp = builtins.open(filename, "rb")
FileNotFoundError: [Errno 2] No such file or directory: '/scratch/project_2014099/data-lmdrive/data/Town10/routes_town10_tiny_w2_09_10_08_57_58/rgb_full/0145.jpg'

2026-01-17 14:36:37,987 [INFO] /scratch/project_2014099/data-lmdrive/data/Town10/routes_town10_tiny_w14_09_10_18_46_50/rgb_full/0137.jpg
2026-01-17 14:36:37,991 [INFO] /scratch/project_2014099/data-lmdrive/data/Town10/routes_town10_tiny_w18_09_09_21_23_11/rgb_full/0054.jpg
2026-01-17 14:36:37,991 [INFO] /scratch/project_2014099/data-lmdrive/data/Town10/routes_town10_tiny_w4_09_10_09_50_24/rgb_full/0015.jpg
2026-01-17 14:36:37,994 [INFO] /scratch/project_2014099/data-lmdrive/data/Town10/routes_town10_tiny_w1_09_09_18_51_51/rgb_full/0306.jpg
2026-01-17 14:36:37,995 [INFO] /scratch/project_2014099/data-lmdrive/data/Town10/routes_town10_tiny_w2_09_10_14_05_51/rgb_full/0185.jpg
2026-01-17 14:36:38,004 [INFO] /scratch/project_2014099/data-lmdrive/data/Town10/routes_town10_tiny_w6_09_10_04_18_30/rgb_full/0015.jpg
2026-01-17 14:36:38,006 [INFO] /scratch/project_2014099/data-lmdrive/data/Town10/routes_town10_tiny_w10_09_10_06_39_54/rgb_full/0322.jpg
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2685613) of binary: /MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/bin/python
Traceback (most recent call last):
  File "/projappl/project_2014099/envlmdrive/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/projappl/project_2014099/lmdrive-original/LAVIS/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2026-01-17_14:36:40
  host      : g1101.mahti.csc.fi
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2685613)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: g1101: task 0: Exited with exit code 1
srun: Terminating StepId=5811900.0
