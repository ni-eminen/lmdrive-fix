master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
| distributed init (rank 0, world 2): env://
| distributed init (rank 1, world 2): env://
2026-01-22 17:17:56,185 [INFO] 
=====  Running Parameters    =====
2026-01-22 17:17:56,186 [INFO] {
    "amp": true,
    "batch_size_eval": 2,
    "batch_size_train": 2,
    "device": "cuda",
    "dist_backend": "nccl",
    "dist_url": "env://",
    "distributed": true,
    "evaluate": false,
    "gpu": 0,
    "init_lr": 0.0001,
    "lr_sched": "linear_warmup_cosine_lr",
    "max_epoch": 15,
    "min_lr": 1e-05,
    "num_workers": 24,
    "output_dir": "output/drivegpt/cvpr/",
    "rank": 0,
    "resume_ckpt_path": null,
    "seed": 42,
    "task": "carla_drive",
    "train_splits": [
        "train"
    ],
    "valid_splits": [
        "val"
    ],
    "warmup_lr": 1e-06,
    "warmup_steps": 2000,
    "weight_decay": 0.06,
    "world_size": 2
}
2026-01-22 17:17:56,186 [INFO] 
======  Dataset Attributes  ======
2026-01-22 17:17:56,186 [INFO] 
======== carla_voice =======
2026-01-22 17:17:56,187 [INFO] {
    "build_info": {
        "annotations": {
            "train": {
                "enable_notice": true,
                "enable_start_frame_augment": true,
                "scale": [
                    0.95,
                    1.05
                ],
                "storage": "/scratch/project_2014099/data-lmdrive/data/Town10",
                "token_max_length": 40,
                "towns": [
                    1,
                    2,
                    3,
                    4,
                    5,
                    6,
                    7,
                    10
                ],
                "weathers": [
                    0,
                    1,
                    2,
                    3,
                    4,
                    5,
                    6,
                    7,
                    8,
                    9,
                    10,
                    11,
                    14,
                    15,
                    16,
                    17,
                    18,
                    19
                ]
            },
            "val": {
                "enable_notice": true,
                "enable_start_frame_augment": true,
                "scale": [
                    0.95,
                    1.05
                ],
                "storage": "/scratch/project_2014099/data-lmdrive/data/Town10",
                "token_max_length": 40,
                "towns": [
                    1,
                    2,
                    3,
                    4,
                    5,
                    6,
                    7,
                    10
                ],
                "weathers": [
                    12,
                    13,
                    20
                ]
            }
        }
    }
}
2026-01-22 17:17:56,187 [INFO] 
======  Model Attributes  ======
2026-01-22 17:17:56,187 [INFO] {
    "arch": "vicuna_drive",
    "drop_path_rate": 0,
    "finetuned": "",
    "freeze_vit": true,
    "image_size": 224,
    "llm_model": "bczhou/TinyLLaVA-2.0B",
    "load_finetuned": false,
    "load_pretrained": true,
    "max_txt_len": 64,
    "model_type": "vicuna7b",
    "num_query_token": 32,
    "preception_model": "memfuser_baseline_e1d3_return_feature",
    "preception_model_ckpt": "../vision_encoder/sensor_pretrain.pth.tar.r50",
    "pretrained": "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/InstructBLIP/instruct_blip_vicuna7b_trimmed.pth",
    "prompt": "",
    "split_section_num_for_visual_encoder": 2,
    "use_extra_prompt": false,
    "use_grad_checkpoint": false,
    "use_notice_prompt": true,
    "vit_precision": "fp16"
}
2026-01-22 17:17:56,204 [INFO] Scenario nums: 41
2026-01-22 17:17:56,218 [INFO] Scenario nums: 29
/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
2026-01-22 17:17:56,775 [INFO] Lock 140731614203808 acquired on /users/matiasni/.cache/huggingface/hub/.locks/models--bert-base-uncased/fb140275c155a9c7c5a3b3e0e77a9e839594a938.lock
2026-01-22 17:17:56,776 [INFO] Lock 140731614203808 released on /users/matiasni/.cache/huggingface/hub/.locks/models--bert-base-uncased/fb140275c155a9c7c5a3b3e0e77a9e839594a938.lock
2026-01-22 17:17:57,365 [INFO] Lock 140731644218528 acquired on /users/matiasni/.cache/huggingface/hub/.locks/models--bert-base-uncased/e5c73d8a50df1f56fb5b0b8002d7cf4010afdccb.lock
2026-01-22 17:17:57,366 [INFO] Lock 140731644218528 released on /users/matiasni/.cache/huggingface/hub/.locks/models--bert-base-uncased/e5c73d8a50df1f56fb5b0b8002d7cf4010afdccb.lock
2026-01-22 17:17:57,718 [INFO] Lock 140731614202224 acquired on /users/matiasni/.cache/huggingface/hub/.locks/models--bert-base-uncased/45a2321a7ecfdaaf60a6c1fd7f5463994cc8907d.lock
2026-01-22 17:17:57,718 [INFO] Lock 140731614202224 released on /users/matiasni/.cache/huggingface/hub/.locks/models--bert-base-uncased/45a2321a7ecfdaaf60a6c1fd7f5463994cc8907d.lock
2026-01-22 17:17:58,034 [INFO] Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet50d_ra2-464e36ba.pth)
/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/torch/nn/modules/conv.py:459: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)
  return F.conv2d(input, weight, bias, self.stride,
/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/torch/nn/modules/conv.py:459: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)
  return F.conv2d(input, weight, bias, self.stride,
2026-01-22 17:18:01,554 [INFO] freeze vision encoder
2026-01-22 17:18:02,003 [INFO] Lock 140736394931744 acquired on /users/matiasni/.cache/huggingface/hub/.locks/models--bczhou--TinyLLaVA-3.1B/5545fb7cf0935d5c48abc8c28e7475f46c0046e7.lock
2026-01-22 17:18:02,004 [INFO] Lock 140736394931744 released on /users/matiasni/.cache/huggingface/hub/.locks/models--bczhou--TinyLLaVA-3.1B/5545fb7cf0935d5c48abc8c28e7475f46c0046e7.lock
2026-01-22 17:18:02,564 [INFO] Lock 140731644249808 acquired on /users/matiasni/.cache/huggingface/hub/.locks/models--bczhou--TinyLLaVA-3.1B/84ef7fb594b5c0979e48bdeddb60a0adef33df0b.lock
2026-01-22 17:18:02,935 [INFO] Lock 140731644249808 released on /users/matiasni/.cache/huggingface/hub/.locks/models--bczhou--TinyLLaVA-3.1B/84ef7fb594b5c0979e48bdeddb60a0adef33df0b.lock
2026-01-22 17:18:03,236 [INFO] Lock 140731604887536 acquired on /users/matiasni/.cache/huggingface/hub/.locks/models--bczhou--TinyLLaVA-3.1B/226b0752cac7789c48f0cb3ec53eda48b7be36cc.lock
2026-01-22 17:18:03,536 [INFO] Lock 140731604887536 released on /users/matiasni/.cache/huggingface/hub/.locks/models--bczhou--TinyLLaVA-3.1B/226b0752cac7789c48f0cb3ec53eda48b7be36cc.lock
2026-01-22 17:18:03,830 [INFO] Lock 140731604919632 acquired on /users/matiasni/.cache/huggingface/hub/.locks/models--bczhou--TinyLLaVA-3.1B/7debb4784a7d53328d4d021fc46314bec4af3833.lock
2026-01-22 17:18:03,984 [INFO] Lock 140731604919632 released on /users/matiasni/.cache/huggingface/hub/.locks/models--bczhou--TinyLLaVA-3.1B/7debb4784a7d53328d4d021fc46314bec4af3833.lock
2026-01-22 17:18:04,323 [INFO] Lock 140731644249808 acquired on /users/matiasni/.cache/huggingface/hub/.locks/models--bczhou--TinyLLaVA-3.1B/330140f0678dc92ae683e1c1cccffc6a001251ae.lock
2026-01-22 17:18:04,475 [INFO] Lock 140731644249808 released on /users/matiasni/.cache/huggingface/hub/.locks/models--bczhou--TinyLLaVA-3.1B/330140f0678dc92ae683e1c1cccffc6a001251ae.lock
2026-01-22 17:18:04,732 [INFO] Lock 140731604888688 acquired on /users/matiasni/.cache/huggingface/hub/.locks/models--bczhou--TinyLLaVA-2.0B/760f5f0a19b3910b646172053239bc407820d500.lock
2026-01-22 17:18:04,927 [INFO] Lock 140731604888688 released on /users/matiasni/.cache/huggingface/hub/.locks/models--bczhou--TinyLLaVA-2.0B/760f5f0a19b3910b646172053239bc407820d500.lock
2026-01-22 17:18:05,094 [INFO] Lock 140731604919824 acquired on /users/matiasni/.cache/huggingface/hub/.locks/models--bczhou--TinyLLaVA-2.0B/6ea6d36d10e10fdae9b90f56a961fdde5cb8795ebc26408f5351877d129d18dd.lock
2026-01-22 17:19:19,505 [INFO] Lock 140731604919824 released on /users/matiasni/.cache/huggingface/hub/.locks/models--bczhou--TinyLLaVA-2.0B/6ea6d36d10e10fdae9b90f56a961fdde5cb8795ebc26408f5351877d129d18dd.lock
Loading Q-Former
2026-01-22 17:19:37,467 [INFO] Lock 140731604754096 acquired on /users/matiasni/.cache/huggingface/hub/.locks/models--bert-base-uncased/68d45e234eb4a928074dfd868cead0219ab85354cc53d20e772753c6bb9169d3.lock
2026-01-22 17:19:37,468 [INFO] Lock 140731604754096 released on /users/matiasni/.cache/huggingface/hub/.locks/models--bert-base-uncased/68d45e234eb4a928074dfd868cead0219ab85354cc53d20e772753c6bb9169d3.lock
2026-01-22 17:19:39,858 [INFO] Start training
2026-01-22 17:19:40,723 [INFO] dataset_ratios not specified, datasets will be concatenated (map-style datasets) or chained (webdataset.DataPipeline).
2026-01-22 17:19:40,723 [INFO] Loaded 41 records for train split from the dataset.
41
29
2026-01-22 17:19:40,723 [INFO] Loaded 29 records for val split from the dataset.
/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 24 worker processes in total. Our suggested max number of worker in current system is 10, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 24 worker processes in total. Our suggested max number of worker in current system is 10, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
trainable parameter shape:  torch.Size([1, 4, 768])
trainable parameter shape:  torch.Size([2048, 2048])
trainable parameter shape:  torch.Size([10, 2048])
trainable parameter shape:  torch.Size([2048, 2048])
trainable parameter shape:  torch.Size([2, 2048])
trainable parameter shape:  torch.Size([50297, 768])
trainable parameter shape:  torch.Size([512, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([2048, 768])
trainable parameter shape:  torch.Size([256])
trainable parameter shape:  torch.Size([256])
trainable parameter shape:  torch.Size([2048])
trainable parameter shape:  torch.Size([10])
trainable parameter shape:  torch.Size([2048])
trainable parameter shape:  torch.Size([2])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([2048])
2026-01-22 17:19:40,729 [INFO] number of trainable parameters: 200225548
2026-01-22 17:19:40,729 [INFO] Start training epoch 0, 10 iters per inner epoch.
Train: data epoch: [0]  [ 0/10]  eta: 0:02:16  lr: 0.000001  loss: 3.5575  waypoints_loss: 3.4003 (3.4003)  end_loss: 0.7860 (0.7860)  end_acc: 0.4125 (0.4125)  time: 13.6980  data: 0.0000  max mem: 16995
2026-01-22 17:19:54,444 [INFO] Reducer buckets have been rebuilt in this iteration.
Train: data epoch: [0]  [ 9/10]  eta: 0:00:01  lr: 0.000001  loss: 4.2778  waypoints_loss: 3.4217 (3.8436)  end_loss: 1.0241 (1.0894)  end_acc: 0.3250 (0.3668)  time: 1.9455  data: 0.0000  max mem: 18384
Train: data epoch: [0] Total time: 0:00:19 (1.9460 s / it)
2026-01-22 17:20:00,194 [INFO] Evaluating on val.
Evaluation  [0/8]  eta: 0:00:51    time: 6.4591  data: 5.9250  max mem: 18701
Evaluation  [7/8]  eta: 0:00:01    time: 1.2727  data: 0.7751  max mem: 18701
Evaluation Total time: 0:00:10 (1.2730 s / it)
2026-01-22 17:20:10,878 [INFO] Eval Epoch 0, loss: 4.634, waypoints_loss: 4.442, end_loss: 0.957, end_acc: 0.279, 
2026-01-22 17:20:10,899 [INFO] Saving checkpoint at epoch 0 to /projappl/project_2014099/lmdrive-original/LAVIS/lavis/output/drivegpt/cvpr/20260122171755/checkpoint_best.pth.
2026-01-22 17:20:15,161 [INFO] Start training
2026-01-22 17:20:15,183 [INFO] Start training epoch 1, 10 iters per inner epoch.
Train: data epoch: [1]  [ 0/10]  eta: 0:01:44  lr: 0.000099  loss: 4.3323  waypoints_loss: 4.1560 (4.1560)  end_loss: 0.8816 (0.8816)  end_acc: 0.4000 (0.4000)  time: 10.4730  data: 0.0000  max mem: 18701
Train: data epoch: [1]  [ 9/10]  eta: 0:00:01  lr: 0.000099  loss: 1.9392  waypoints_loss: 2.1502 (2.6658)  end_loss: 0.2364 (0.3164)  end_acc: 0.9683 (0.9135)  time: 1.6260  data: 0.0000  max mem: 18701
Train: data epoch: [1] Total time: 0:00:16 (1.6262 s / it)
2026-01-22 17:20:31,448 [INFO] Evaluating on val.
Evaluation  [0/8]  eta: 0:00:44    time: 5.5840  data: 5.0552  max mem: 18701
Evaluation  [7/8]  eta: 0:00:01    time: 1.1367  data: 0.6671  max mem: 18701
Evaluation Total time: 0:00:09 (1.1370 s / it)
2026-01-22 17:20:41,179 [INFO] Eval Epoch 1, loss: 2.228, waypoints_loss: 2.141, end_loss: 0.433, end_acc: 0.969, 
2026-01-22 17:20:41,201 [INFO] Saving checkpoint at epoch 1 to /projappl/project_2014099/lmdrive-original/LAVIS/lavis/output/drivegpt/cvpr/20260122171755/checkpoint_best.pth.
2026-01-22 17:20:46,295 [INFO] Start training
2026-01-22 17:20:46,316 [INFO] Start training epoch 2, 10 iters per inner epoch.
Train: data epoch: [2]  [ 0/10]  eta: 0:01:36  lr: 0.000096  loss: 1.8883  waypoints_loss: 1.8218 (1.8218)  end_loss: 0.3325 (0.3325)  end_acc: 0.9718 (0.9718)  time: 9.6465  data: 0.0000  max mem: 18701
Train: data epoch: [2]  [ 9/10]  eta: 0:00:01  lr: 0.000096  loss: 2.6060  waypoints_loss: 2.0833 (2.0688)  end_loss: 0.2269 (0.2493)  end_acc: 0.9718 (0.9660)  time: 1.5760  data: 0.0000  max mem: 18701
Train: data epoch: [2] Total time: 0:00:15 (1.5762 s / it)
2026-01-22 17:21:02,084 [INFO] Evaluating on val.
Evaluation  [0/8]  eta: 0:00:53    time: 6.6834  data: 6.1628  max mem: 18701
Evaluation  [7/8]  eta: 0:00:01    time: 1.2862  data: 0.8102  max mem: 18701
Evaluation Total time: 0:00:10 (1.2865 s / it)
2026-01-22 17:21:12,389 [INFO] Eval Epoch 2, loss: 1.812, waypoints_loss: 1.778, end_loss: 0.166, end_acc: 0.969, 
2026-01-22 17:21:12,410 [INFO] Saving checkpoint at epoch 2 to /projappl/project_2014099/lmdrive-original/LAVIS/lavis/output/drivegpt/cvpr/20260122171755/checkpoint_best.pth.
2026-01-22 17:21:17,608 [INFO] Start training
2026-01-22 17:21:17,629 [INFO] Start training epoch 3, 10 iters per inner epoch.
Train: data epoch: [3]  [ 0/10]  eta: 0:01:50  lr: 0.000091  loss: 1.6612  waypoints_loss: 1.6191 (1.6191)  end_loss: 0.2100 (0.2100)  end_acc: 0.9750 (0.9750)  time: 11.0415  data: 0.0000  max mem: 18701
Train: data epoch: [3]  [ 9/10]  eta: 0:00:01  lr: 0.000091  loss: 1.6072  waypoints_loss: 1.6191 (1.9357)  end_loss: 0.2055 (0.2057)  end_acc: 0.9718 (0.9708)  time: 1.6769  data: 0.0000  max mem: 18701
Train: data epoch: [3] Total time: 0:00:16 (1.6770 s / it)
2026-01-22 17:21:34,403 [INFO] Evaluating on val.
Evaluation  [0/8]  eta: 0:00:41    time: 5.2179  data: 4.6809  max mem: 18701
Evaluation  [7/8]  eta: 0:00:01    time: 1.2011  data: 0.7291  max mem: 18701
Evaluation Total time: 0:00:09 (1.2014 s / it)
2026-01-22 17:21:44,085 [INFO] Eval Epoch 3, loss: 1.797, waypoints_loss: 1.742, end_loss: 0.278, end_acc: 0.968, 
2026-01-22 17:21:44,106 [INFO] Saving checkpoint at epoch 3 to /projappl/project_2014099/lmdrive-original/LAVIS/lavis/output/drivegpt/cvpr/20260122171755/checkpoint_best.pth.
2026-01-22 17:21:49,301 [INFO] Start training
2026-01-22 17:21:49,323 [INFO] Start training epoch 4, 10 iters per inner epoch.
Train: data epoch: [4]  [ 0/10]  eta: 0:01:39  lr: 0.000085  loss: 1.2454  waypoints_loss: 1.1372 (1.1372)  end_loss: 0.5409 (0.5409)  end_acc: 0.9375 (0.9375)  time: 9.9984  data: 0.0000  max mem: 18701
Train: data epoch: [4]  [ 9/10]  eta: 0:00:01  lr: 0.000085  loss: 1.6824  waypoints_loss: 1.6567 (1.8769)  end_loss: 0.1512 (0.1932)  end_acc: 0.9740 (0.9678)  time: 1.5700  data: 0.0000  max mem: 18701
Train: data epoch: [4] Total time: 0:00:15 (1.5701 s / it)
2026-01-22 17:22:05,030 [INFO] Evaluating on val.
Evaluation  [0/8]  eta: 0:00:47    time: 5.9212  data: 5.3977  max mem: 18701
Evaluation  [7/8]  eta: 0:00:01    time: 1.1756  data: 0.7039  max mem: 18701
Evaluation Total time: 0:00:09 (1.1759 s / it)
2026-01-22 17:22:14,697 [INFO] Eval Epoch 4, loss: 1.765, waypoints_loss: 1.730, end_loss: 0.175, end_acc: 0.969, 
2026-01-22 17:22:14,718 [INFO] Saving checkpoint at epoch 4 to /projappl/project_2014099/lmdrive-original/LAVIS/lavis/output/drivegpt/cvpr/20260122171755/checkpoint_best.pth.
2026-01-22 17:22:19,915 [INFO] Start training
2026-01-22 17:22:19,936 [INFO] Start training epoch 5, 10 iters per inner epoch.
Train: data epoch: [5]  [ 0/10]  eta: 0:01:35  lr: 0.000077  loss: 1.4989  waypoints_loss: 1.4636 (1.4636)  end_loss: 0.1764 (0.1764)  end_acc: 0.9600 (0.9600)  time: 9.5149  data: 0.0000  max mem: 18701
Train: data epoch: [5]  [ 9/10]  eta: 0:00:01  lr: 0.000077  loss: 2.4978  waypoints_loss: 1.6093 (1.8413)  end_loss: 0.1339 (0.1381)  end_acc: 0.9747 (0.9711)  time: 1.5502  data: 0.0000  max mem: 18701
Train: data epoch: [5] Total time: 0:00:15 (1.5504 s / it)
2026-01-22 17:22:35,444 [INFO] Evaluating on val.
Evaluation  [0/8]  eta: 0:00:49    time: 6.1933  data: 5.6639  max mem: 18701
Evaluation  [7/8]  eta: 0:00:01    time: 1.2230  data: 0.7535  max mem: 18701
Evaluation Total time: 0:00:09 (1.2233 s / it)
2026-01-22 17:22:45,245 [INFO] Eval Epoch 5, loss: 1.677, waypoints_loss: 1.642, end_loss: 0.176, end_acc: 0.969, 
2026-01-22 17:22:45,265 [INFO] Saving checkpoint at epoch 5 to /projappl/project_2014099/lmdrive-original/LAVIS/lavis/output/drivegpt/cvpr/20260122171755/checkpoint_best.pth.
2026-01-22 17:22:50,489 [INFO] Start training
2026-01-22 17:22:50,511 [INFO] Start training epoch 6, 10 iters per inner epoch.
Train: data epoch: [6]  [ 0/10]  eta: 0:01:55  lr: 0.000069  loss: 3.1015  waypoints_loss: 3.0755 (3.0755)  end_loss: 0.1300 (0.1300)  end_acc: 0.9747 (0.9747)  time: 11.5346  data: 0.0000  max mem: 18701
Train: data epoch: [6]  [ 9/10]  eta: 0:00:01  lr: 0.000069  loss: 2.5785  waypoints_loss: 1.1813 (1.6140)  end_loss: 0.1300 (0.1374)  end_acc: 0.9733 (0.9701)  time: 1.7233  data: 0.0000  max mem: 18701
Train: data epoch: [6] Total time: 0:00:17 (1.7234 s / it)
2026-01-22 17:23:07,749 [INFO] Evaluating on val.
Evaluation  [0/8]  eta: 0:00:53    time: 6.6354  data: 6.1092  max mem: 18701
Evaluation  [7/8]  eta: 0:00:01    time: 1.2786  data: 0.8088  max mem: 18701
Evaluation Total time: 0:00:10 (1.2789 s / it)
2026-01-22 17:23:17,994 [INFO] Eval Epoch 6, loss: 1.643, waypoints_loss: 1.610, end_loss: 0.163, end_acc: 0.969, 
2026-01-22 17:23:18,015 [INFO] Saving checkpoint at epoch 6 to /projappl/project_2014099/lmdrive-original/LAVIS/lavis/output/drivegpt/cvpr/20260122171755/checkpoint_best.pth.
2026-01-22 17:23:23,227 [INFO] Start training
2026-01-22 17:23:23,249 [INFO] Start training epoch 7, 10 iters per inner epoch.
Train: data epoch: [7]  [ 0/10]  eta: 0:01:53  lr: 0.000060  loss: 0.9833  waypoints_loss: 0.9509 (0.9509)  end_loss: 0.1624 (0.1624)  end_acc: 0.9677 (0.9677)  time: 11.3940  data: 0.0000  max mem: 18701
Train: data epoch: [7]  [ 9/10]  eta: 0:00:01  lr: 0.000060  loss: 1.3094  waypoints_loss: 1.2961 (1.4842)  end_loss: 0.1504 (0.1584)  end_acc: 0.9672 (0.9643)  time: 1.7089  data: 0.0000  max mem: 18701
Train: data epoch: [7] Total time: 0:00:17 (1.7091 s / it)
2026-01-22 17:23:40,344 [INFO] Evaluating on val.
Evaluation  [0/8]  eta: 0:00:50    time: 6.2554  data: 5.7130  max mem: 18701
Evaluation  [7/8]  eta: 0:00:01    time: 1.2268  data: 0.7565  max mem: 18701
Evaluation Total time: 0:00:09 (1.2271 s / it)
2026-01-22 17:23:50,175 [INFO] Eval Epoch 7, loss: 1.635, waypoints_loss: 1.607, end_loss: 0.140, end_acc: 0.969, 
2026-01-22 17:23:50,196 [INFO] Saving checkpoint at epoch 7 to /projappl/project_2014099/lmdrive-original/LAVIS/lavis/output/drivegpt/cvpr/20260122171755/checkpoint_best.pth.
2026-01-22 17:23:55,550 [INFO] Start training
2026-01-22 17:23:55,572 [INFO] Start training epoch 8, 10 iters per inner epoch.
Train: data epoch: [8]  [ 0/10]  eta: 0:01:37  lr: 0.000050  loss: 1.4402  waypoints_loss: 1.4172 (1.4172)  end_loss: 0.1149 (0.1149)  end_acc: 0.9726 (0.9726)  time: 9.7364  data: 0.0000  max mem: 18701
Train: data epoch: [8]  [ 9/10]  eta: 0:00:01  lr: 0.000050  loss: 2.0354  waypoints_loss: 1.5431 (1.6904)  end_loss: 0.1168 (0.1451)  end_acc: 0.9672 (0.9682)  time: 1.5726  data: 0.0000  max mem: 18701
Train: data epoch: [8] Total time: 0:00:15 (1.5727 s / it)
2026-01-22 17:24:11,303 [INFO] Evaluating on val.
Evaluation  [0/8]  eta: 0:00:45    time: 5.6928  data: 5.0882  max mem: 18701
Evaluation  [7/8]  eta: 0:00:01    time: 1.1634  data: 0.6837  max mem: 18701
Evaluation Total time: 0:00:09 (1.1637 s / it)
2026-01-22 17:24:20,627 [INFO] Eval Epoch 8, loss: 1.629, waypoints_loss: 1.604, end_loss: 0.124, end_acc: 0.969, 
2026-01-22 17:24:20,647 [INFO] Saving checkpoint at epoch 8 to /projappl/project_2014099/lmdrive-original/LAVIS/lavis/output/drivegpt/cvpr/20260122171755/checkpoint_best.pth.
2026-01-22 17:24:25,862 [INFO] Start training
2026-01-22 17:24:25,884 [INFO] Start training epoch 9, 10 iters per inner epoch.
Train: data epoch: [9]  [ 0/10]  eta: 0:01:45  lr: 0.000041  loss: 1.9378  waypoints_loss: 1.9175 (1.9175)  end_loss: 0.1017 (0.1017)  end_acc: 0.9750 (0.9750)  time: 10.5846  data: 0.0000  max mem: 18701
Train: data epoch: [9]  [ 9/10]  eta: 0:00:01  lr: 0.000041  loss: 2.1046  waypoints_loss: 1.8820 (1.7374)  end_loss: 0.1164 (0.1210)  end_acc: 0.9737 (0.9715)  time: 1.6303  data: 0.0000  max mem: 18701
Train: data epoch: [9] Total time: 0:00:16 (1.6305 s / it)
2026-01-22 17:24:42,193 [INFO] Evaluating on val.
Evaluation  [0/8]  eta: 0:00:45    time: 5.7322  data: 5.2064  max mem: 18701
Evaluation  [7/8]  eta: 0:00:01    time: 1.1578  data: 0.6794  max mem: 18701
Evaluation Total time: 0:00:09 (1.1596 s / it)
2026-01-22 17:24:51,824 [INFO] Eval Epoch 9, loss: 1.656, waypoints_loss: 1.631, end_loss: 0.124, end_acc: 0.969, 
2026-01-22 17:24:51,826 [INFO] Start training
2026-01-22 17:24:51,848 [INFO] Start training epoch 10, 10 iters per inner epoch.
Train: data epoch: [10]  [ 0/10]  eta: 0:01:39  lr: 0.000033  loss: 1.4379  waypoints_loss: 1.4170 (1.4170)  end_loss: 0.1047 (0.1047)  end_acc: 0.9750 (0.9750)  time: 9.9212  data: 0.0000  max mem: 18701
Train: data epoch: [10]  [ 9/10]  eta: 0:00:01  lr: 0.000033  loss: 0.8028  waypoints_loss: 1.2497 (1.4432)  end_loss: 0.1108 (0.1181)  end_acc: 0.9710 (0.9706)  time: 1.5645  data: 0.0000  max mem: 18701
Train: data epoch: [10] Total time: 0:00:15 (1.5646 s / it)
2026-01-22 17:25:07,500 [INFO] Evaluating on val.
Evaluation  [0/8]  eta: 0:00:46    time: 5.8481  data: 5.3225  max mem: 18701
Evaluation  [7/8]  eta: 0:00:01    time: 1.1641  data: 0.6945  max mem: 18701
Evaluation Total time: 0:00:09 (1.1644 s / it)
2026-01-22 17:25:17,664 [INFO] Eval Epoch 10, loss: 1.541, waypoints_loss: 1.517, end_loss: 0.123, end_acc: 0.969, 
2026-01-22 17:25:17,685 [INFO] Saving checkpoint at epoch 10 to /projappl/project_2014099/lmdrive-original/LAVIS/lavis/output/drivegpt/cvpr/20260122171755/checkpoint_best.pth.
2026-01-22 17:25:22,882 [INFO] Start training
2026-01-22 17:25:22,904 [INFO] Start training epoch 11, 10 iters per inner epoch.
Train: data epoch: [11]  [ 0/10]  eta: 0:01:36  lr: 0.000025  loss: 1.0188  waypoints_loss: 0.9940 (0.9940)  end_loss: 0.1241 (0.1241)  end_acc: 0.9677 (0.9677)  time: 9.6410  data: 0.0000  max mem: 18701
Train: data epoch: [11]  [ 9/10]  eta: 0:00:01  lr: 0.000025  loss: 2.1694  waypoints_loss: 1.3767 (1.7587)  end_loss: 0.1082 (0.1199)  end_acc: 0.9688 (0.9696)  time: 1.6471  data: 0.0000  max mem: 18701
Train: data epoch: [11] Total time: 0:00:16 (1.6472 s / it)
2026-01-22 17:25:39,381 [INFO] Evaluating on val.
Evaluation  [0/8]  eta: 0:00:53    time: 6.6941  data: 6.1739  max mem: 18701
Evaluation  [7/8]  eta: 0:00:01    time: 1.2822  data: 0.8142  max mem: 18701
Evaluation Total time: 0:00:10 (1.2825 s / it)
2026-01-22 17:25:49,654 [INFO] Eval Epoch 11, loss: 1.553, waypoints_loss: 1.530, end_loss: 0.116, end_acc: 0.969, 
2026-01-22 17:25:49,656 [INFO] Start training
2026-01-22 17:25:49,678 [INFO] Start training epoch 12, 10 iters per inner epoch.
Train: data epoch: [12]  [ 0/10]  eta: 0:01:48  lr: 0.000019  loss: 1.1668  waypoints_loss: 1.1461 (1.1461)  end_loss: 0.1038 (0.1038)  end_acc: 0.9750 (0.9750)  time: 10.8684  data: 0.0000  max mem: 18701
Train: data epoch: [12]  [ 9/10]  eta: 0:00:01  lr: 0.000019  loss: 1.1755  waypoints_loss: 1.1884 (1.5991)  end_loss: 0.1188 (0.1237)  end_acc: 0.9672 (0.9682)  time: 1.6586  data: 0.0000  max mem: 18701
Train: data epoch: [12] Total time: 0:00:16 (1.6587 s / it)
2026-01-22 17:26:06,269 [INFO] Evaluating on val.
Evaluation  [0/8]  eta: 0:00:45    time: 5.6790  data: 5.1368  max mem: 18701
Evaluation  [7/8]  eta: 0:00:01    time: 1.1579  data: 0.6865  max mem: 18701
Evaluation Total time: 0:00:09 (1.1582 s / it)
2026-01-22 17:26:15,623 [INFO] Eval Epoch 12, loss: 1.579, waypoints_loss: 1.555, end_loss: 0.119, end_acc: 0.969, 
2026-01-22 17:26:15,625 [INFO] Start training
2026-01-22 17:26:15,647 [INFO] Start training epoch 13, 10 iters per inner epoch.
Train: data epoch: [13]  [ 0/10]  eta: 0:01:39  lr: 0.000014  loss: 1.3835  waypoints_loss: 1.3454 (1.3454)  end_loss: 0.1905 (0.1905)  end_acc: 0.9512 (0.9512)  time: 9.9979  data: 0.0000  max mem: 18701
Train: data epoch: [13]  [ 9/10]  eta: 0:00:01  lr: 0.000014  loss: 0.9180  waypoints_loss: 1.3047 (1.5117)  end_loss: 0.1027 (0.1207)  end_acc: 0.9750 (0.9701)  time: 1.6122  data: 0.0000  max mem: 18701
Train: data epoch: [13] Total time: 0:00:16 (1.6123 s / it)
2026-01-22 17:26:31,776 [INFO] Evaluating on val.
Evaluation  [0/8]  eta: 0:00:49    time: 6.1884  data: 5.6616  max mem: 18701
Evaluation  [7/8]  eta: 0:00:01    time: 1.2076  data: 0.7392  max mem: 18701
Evaluation Total time: 0:00:09 (1.2079 s / it)
2026-01-22 17:26:42,341 [INFO] Eval Epoch 13, loss: 1.607, waypoints_loss: 1.583, end_loss: 0.120, end_acc: 0.969, 
2026-01-22 17:26:42,343 [INFO] Start training
2026-01-22 17:26:42,365 [INFO] Start training epoch 14, 10 iters per inner epoch.
Train: data epoch: [14]  [ 0/10]  eta: 0:01:51  lr: 0.000011  loss: 1.2376  waypoints_loss: 1.2172 (1.2172)  end_loss: 0.1018 (0.1018)  end_acc: 0.9750 (0.9750)  time: 11.1275  data: 0.0000  max mem: 18701
Train: data epoch: [14]  [ 9/10]  eta: 0:00:01  lr: 0.000011  loss: 1.2754  waypoints_loss: 1.2477 (1.3234)  end_loss: 0.1019 (0.1297)  end_acc: 0.9722 (0.9667)  time: 1.7292  data: 0.0000  max mem: 18701
Train: data epoch: [14] Total time: 0:00:17 (1.7293 s / it)
2026-01-22 17:26:59,663 [INFO] Evaluating on val.
Evaluation  [0/8]  eta: 0:00:48    time: 6.0058  data: 5.4764  max mem: 18701
Evaluation  [7/8]  eta: 0:00:01    time: 1.1873  data: 0.7180  max mem: 18701
Evaluation Total time: 0:00:09 (1.1876 s / it)
2026-01-22 17:27:09,480 [INFO] Eval Epoch 14, loss: 1.620, waypoints_loss: 1.595, end_loss: 0.122, end_acc: 0.968, 
2026-01-22 17:27:09,483 [INFO] Training time 0:07:29
