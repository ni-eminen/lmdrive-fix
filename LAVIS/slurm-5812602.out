master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
| distributed init (rank 0, world 1): env://
2026-01-17 17:11:45,911 [INFO] 
=====  Running Parameters    =====
2026-01-17 17:11:45,911 [INFO] {
    "amp": true,
    "batch_size_eval": 4,
    "batch_size_train": 4,
    "device": "cuda",
    "dist_backend": "nccl",
    "dist_url": "env://",
    "distributed": true,
    "evaluate": false,
    "gpu": 0,
    "init_lr": 0.0001,
    "lr_sched": "linear_warmup_cosine_lr",
    "max_epoch": 15,
    "min_lr": 1e-05,
    "num_workers": 24,
    "output_dir": "output/drivegpt/cvpr/",
    "rank": 0,
    "resume_ckpt_path": null,
    "seed": 42,
    "task": "carla_drive",
    "train_splits": [
        "train"
    ],
    "valid_splits": [
        "val"
    ],
    "warmup_lr": 1e-06,
    "warmup_steps": 2000,
    "weight_decay": 0.06,
    "world_size": 1
}
2026-01-17 17:11:45,911 [INFO] 
======  Dataset Attributes  ======
2026-01-17 17:11:45,912 [INFO] 
======== carla_voice =======
2026-01-17 17:11:45,912 [INFO] {
    "build_info": {
        "annotations": {
            "train": {
                "enable_notice": true,
                "enable_start_frame_augment": true,
                "scale": [
                    0.95,
                    1.05
                ],
                "storage": "/scratch/project_2014099/data-lmdrive/data/Town10",
                "token_max_length": 40,
                "towns": [
                    1,
                    2,
                    3,
                    4,
                    5,
                    6,
                    7,
                    10
                ],
                "weathers": [
                    0,
                    1,
                    2,
                    3,
                    4,
                    5,
                    6,
                    7,
                    8,
                    9,
                    10,
                    11,
                    14,
                    15,
                    16,
                    17,
                    18,
                    19
                ]
            },
            "val": {
                "enable_notice": true,
                "enable_start_frame_augment": true,
                "scale": [
                    0.95,
                    1.05
                ],
                "storage": "/scratch/project_2014099/data-lmdrive/data/Town10",
                "token_max_length": 40,
                "towns": [
                    1,
                    2,
                    3,
                    4,
                    5,
                    6,
                    7,
                    10
                ],
                "weathers": [
                    12,
                    13,
                    20
                ]
            }
        }
    }
}
2026-01-17 17:11:45,912 [INFO] 
======  Model Attributes  ======
2026-01-17 17:11:45,913 [INFO] {
    "arch": "vicuna_drive",
    "drop_path_rate": 0,
    "finetuned": "",
    "freeze_vit": true,
    "image_size": 224,
    "llm_model": "bczhou/TinyLLaVA-2.0B",
    "load_finetuned": false,
    "load_pretrained": true,
    "max_txt_len": 64,
    "model_type": "vicuna7b",
    "num_query_token": 32,
    "preception_model": "memfuser_baseline_e1d3_return_feature",
    "preception_model_ckpt": "../vision_encoder/sensor_pretrain.pth.tar.r50",
    "pretrained": "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/InstructBLIP/instruct_blip_vicuna7b_trimmed.pth",
    "prompt": "",
    "split_section_num_for_visual_encoder": 2,
    "use_extra_prompt": false,
    "use_grad_checkpoint": false,
    "use_notice_prompt": true,
    "vit_precision": "fp16"
}
2026-01-17 17:11:45,918 [INFO] Scenario nums: 41
2026-01-17 17:11:45,935 [INFO] Scenario nums: 29
/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
2026-01-17 17:11:46,538 [INFO] Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet50d_ra2-464e36ba.pth)
/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/torch/nn/modules/conv.py:459: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)
  return F.conv2d(input, weight, bias, self.stride,
2026-01-17 17:11:51,313 [INFO] freeze vision encoder
Loading Q-Former
2026-01-17 17:13:23,399 [INFO] Start training
2026-01-17 17:13:23,948 [INFO] dataset_ratios not specified, datasets will be concatenated (map-style datasets) or chained (webdataset.DataPipeline).
41
29
2026-01-17 17:13:23,948 [INFO] Loaded 41 records for train split from the dataset.
2026-01-17 17:13:23,948 [INFO] Loaded 29 records for val split from the dataset.
/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 24 worker processes in total. Our suggested max number of worker in current system is 10, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
trainable parameter shape:  torch.Size([1, 4, 768])
trainable parameter shape:  torch.Size([2048, 2048])
trainable parameter shape:  torch.Size([10, 2048])
trainable parameter shape:  torch.Size([2048, 2048])
trainable parameter shape:  torch.Size([2, 2048])
trainable parameter shape:  torch.Size([50297, 768])
trainable parameter shape:  torch.Size([512, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([2048, 768])
trainable parameter shape:  torch.Size([256])
trainable parameter shape:  torch.Size([256])
trainable parameter shape:  torch.Size([2048])
trainable parameter shape:  torch.Size([10])
trainable parameter shape:  torch.Size([2048])
trainable parameter shape:  torch.Size([2])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([2048])
2026-01-17 17:13:23,955 [INFO] number of trainable parameters: 200225548
2026-01-17 17:13:23,955 [INFO] Start training epoch 0, 10 iters per inner epoch.
Traceback (most recent call last):
  File "/projappl/project_2014099/lmdrive-original/LAVIS/train.py", line 103, in <module>
    main()
  File "/projappl/project_2014099/lmdrive-original/LAVIS/train.py", line 99, in main
    runner.train()
  File "/projappl/project_2014099/lmdrive-original/LAVIS/lavis/runners/runner_base.py", line 378, in train
    train_stats = self.train_epoch(cur_epoch)
  File "/projappl/project_2014099/lmdrive-original/LAVIS/lavis/runners/runner_base.py", line 437, in train_epoch
    return self.task.train_epoch(
  File "/projappl/project_2014099/lmdrive-original/LAVIS/lavis/tasks/drive.py", line 143, in train_epoch
    return self._train_inner_loop(
  File "/projappl/project_2014099/lmdrive-original/LAVIS/lavis/tasks/drive.py", line 250, in _train_inner_loop
    loss, loss_dict = self.train_step(model=model, samples=samples)
  File "/projappl/project_2014099/lmdrive-original/LAVIS/lavis/tasks/drive.py", line 66, in train_step
    output = model(samples)
  File "/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1156, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1110, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]
  File "/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/projappl/project_2014099/lmdrive-original/LAVIS/lavis/models/drive_models/drive.py", line 453, in forward
    llm_inputs, llm_attention_mask, input_part_targets_len, wp_target_index = self.concat_text_image_input_with_notice(inputs_embeds, text_input_tokens.attention_mask,
  File "/projappl/project_2014099/lmdrive-original/LAVIS/lavis/models/drive_models/drive.py", line 251, in concat_text_image_input_with_notice
    notice_embeds = self.llm_model.get_input_embeddings()(text_input_tokens.input_ids)
  File "/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2757190) of binary: /MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/bin/python
Traceback (most recent call last):
  File "/projappl/project_2014099/envlmdrive/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/projappl/project_2014099/lmdrive-original/LAVIS/train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2026-01-17_17:14:12
  host      : g1101.mahti.csc.fi
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2757190)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: g1101: task 0: Exited with exit code 1
srun: Terminating StepId=5812602.0
