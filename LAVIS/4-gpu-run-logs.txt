master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/diffusers/models/cross_attention.py:30: FutureWarning: Importing from cross_attention is deprecated. Please import from diffusers.models.attention_processor instead.
  deprecate(
| distributed init (rank 0, world 4): env://
| distributed init (rank 3, world 4): env://
| distributed init (rank 2, world 4): env://
| distributed init (rank 1, world 4): env://
2026-01-24 14:47:50,553 [INFO] 
=====  Running Parameters    =====
2026-01-24 14:47:50,553 [INFO] {
    "amp": true,
    "batch_size_eval": 4,
    "batch_size_train": 4,
    "device": "cuda",
    "dist_backend": "nccl",
    "dist_url": "env://",
    "distributed": true,
    "evaluate": false,
    "gpu": 0,
    "init_lr": 0.0001,
    "lr_sched": "linear_warmup_cosine_lr",
    "max_epoch": 15,
    "min_lr": 1e-05,
    "num_workers": 24,
    "output_dir": "output/drivegpt/cvpr/",
    "rank": 0,
    "resume_ckpt_path": null,
    "seed": 42,
    "task": "carla_drive",
    "train_splits": [
        "train"
    ],
    "valid_splits": [
        "val"
    ],
    "warmup_lr": 1e-06,
    "warmup_steps": 2000,
    "weight_decay": 0.06,
    "world_size": 4
}
2026-01-24 14:47:50,553 [INFO] 
======  Dataset Attributes  ======
2026-01-24 14:47:50,554 [INFO] 
======== carla_voice =======
2026-01-24 14:47:50,554 [INFO] {
    "build_info": {
        "annotations": {
            "train": {
                "enable_notice": true,
                "enable_start_frame_augment": true,
                "scale": [
                    0.95,
                    1.05
                ],
                "storage": "/scratch/project_2014099/data-lmdrive/data/Town10",
                "token_max_length": 40,
                "towns": [
                    1,
                    2,
                    3,
                    4,
                    5,
                    6,
                    7,
                    10
                ],
                "weathers": [
                    0,
                    1,
                    2,
                    3,
                    4,
                    5,
                    6,
                    7,
                    8,
                    9,
                    10,
                    11,
                    14,
                    15,
                    16,
                    17,
                    18,
                    19
                ]
            },
            "val": {
                "enable_notice": true,
                "enable_start_frame_augment": true,
                "scale": [
                    0.95,
                    1.05
                ],
                "storage": "/scratch/project_2014099/data-lmdrive/data/Town10",
                "token_max_length": 40,
                "towns": [
                    1,
                    2,
                    3,
                    4,
                    5,
                    6,
                    7,
                    10
                ],
                "weathers": [
                    12,
                    13,
                    20
                ]
            }
        }
    }
}
2026-01-24 14:47:50,554 [INFO] 
======  Model Attributes  ======
2026-01-24 14:47:50,554 [INFO] {
    "arch": "vicuna_drive",
    "drop_path_rate": 0,
    "finetuned": "",
    "freeze_vit": true,
    "image_size": 224,
    "llm_model": "bczhou/TinyLLaVA-2.0B",
    "load_finetuned": false,
    "load_pretrained": true,
    "max_txt_len": 64,
    "model_type": "vicuna7b",
    "num_query_token": 32,
    "preception_model": "memfuser_baseline_e1d3_return_feature",
    "preception_model_ckpt": "../vision_encoder/sensor_pretrain.pth.tar.r50",
    "pretrained": "https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/InstructBLIP/instruct_blip_vicuna7b_trimmed.pth",
    "prompt": "",
    "split_section_num_for_visual_encoder": 2,
    "use_extra_prompt": false,
    "use_grad_checkpoint": false,
    "use_notice_prompt": true,
    "vit_precision": "fp16"
}
2026-01-24 14:47:50,566 [INFO] Scenario nums: 41
2026-01-24 14:47:50,580 [INFO] Scenario nums: 29
2026-01-24 14:47:50,705 [INFO] Crop type is BirdViewCropType.FRONT_AND_REAR_AREA
/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading Q-Former
2026-01-24 14:48:28,620 [INFO] Start training
2026-01-24 14:48:30,131 [INFO] dataset_ratios not specified, datasets will be concatenated (map-style datasets) or chained (webdataset.DataPipeline).
41
29
2026-01-24 14:48:30,131 [INFO] Loaded 41 records for train split from the dataset.
2026-01-24 14:48:30,131 [INFO] Loaded 29 records for val split from the dataset.
/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 24 worker processes in total. Our suggested max number of worker in current system is 10, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 24 worker processes in total. Our suggested max number of worker in current system is 10, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 24 worker processes in total. Our suggested max number of worker in current system is 10, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 24 worker processes in total. Our suggested max number of worker in current system is 10, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
trainable parameter shape:  torch.Size([1, 4, 768])
trainable parameter shape:  torch.Size([2048, 2048])
trainable parameter shape:  torch.Size([10, 2048])
trainable parameter shape:  torch.Size([2048, 2048])
trainable parameter shape:  torch.Size([2, 2048])
trainable parameter shape:  torch.Size([50297, 768])
trainable parameter shape:  torch.Size([512, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 256])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([768, 768])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([3072, 768])
trainable parameter shape:  torch.Size([768, 3072])
trainable parameter shape:  torch.Size([2048, 768])
trainable parameter shape:  torch.Size([2048])
trainable parameter shape:  torch.Size([10])
trainable parameter shape:  torch.Size([2048])
trainable parameter shape:  torch.Size([2])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([3072])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([768])
trainable parameter shape:  torch.Size([2048])
2026-01-24 14:48:30,137 [INFO] number of trainable parameters: 200225036
2026-01-24 14:48:30,137 [INFO] Start training epoch 0, 2 iters per inner epoch.
/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/torch/nn/modules/conv.py:459: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)
  return F.conv2d(input, weight, bias, self.stride,
/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/torch/nn/modules/conv.py:459: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)
  return F.conv2d(input, weight, bias, self.stride,
/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/torch/nn/modules/conv.py:459: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)
  return F.conv2d(input, weight, bias, self.stride,
/MAHTI_TYKKY_9QXaOsY/miniforge/envs/env1/lib/python3.8/site-packages/torch/nn/modules/conv.py:459: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)
  return F.conv2d(input, weight, bias, self.stride,
Train: data epoch: [0]  [0/2]  eta: 0:00:38  lr: 0.000001  loss: 3.7670  waypoints_loss: 3.5080 (3.5080)  end_loss: 1.2952 (1.2952)  end_acc: 0.1972 (0.1972)  time: 19.0526  data: 0.0000  max mem: 14808
2026-01-24 14:48:49,557 [INFO] Reducer buckets have been rebuilt in this iteration.
Train: data epoch: [0]  [1/2]  eta: 0:00:09  lr: 0.000001  loss: 3.6772  waypoints_loss: 3.4454 (3.4767)  end_loss: 1.1589 (1.2271)  end_acc: 0.1972 (0.2197)  time: 9.8980  data: 0.0000  max mem: 14808
Train: data epoch: [0] Total time: 0:00:19 (9.9003 s / it)
2026-01-24 14:48:49,945 [INFO] Evaluating on val.
Evaluation  [0/2]  eta: 0:00:20    time: 10.1699  data: 9.8509  max mem: 14808
Evaluation  [1/2]  eta: 0:00:05    time: 5.3207  data: 5.0249  max mem: 14808
Evaluation Total time: 0:00:10 (5.3219 s / it)
2026-01-24 14:49:02,174 [INFO] Eval Epoch 0, loss: 4.150, waypoints_loss: 3.905, end_loss: 1.225, end_acc: 0.274, 
2026-01-24 14:49:02,188 [INFO] Saving checkpoint at epoch 0 to /projappl/project_2014099/lmdrive-fix/LAVIS/lavis/output/drivegpt/cvpr/20260124144748/checkpoint_best.pth.
2026-01-24 14:49:03,740 [INFO] Start training
2026-01-24 14:49:03,755 [INFO] Start training epoch 1, 2 iters per inner epoch.
Train: data epoch: [1]  [0/2]  eta: 0:00:25  lr: 0.000099  loss: 2.6441  waypoints_loss: 2.4298 (2.4298)  end_loss: 1.0716 (1.0716)  end_acc: 0.3333 (0.3333)  time: 12.6702  data: 0.0000  max mem: 14860
Train: data epoch: [1]  [1/2]  eta: 0:00:06  lr: 0.000099  loss: 3.6266  waypoints_loss: 2.4298 (2.9289)  end_loss: 0.9934 (1.0325)  end_acc: 0.3312 (0.3323)  time: 6.7549  data: 0.0000  max mem: 14860
Train: data epoch: [1] Total time: 0:00:13 (6.7555 s / it)
2026-01-24 14:49:17,272 [INFO] Evaluating on val.
Evaluation  [0/2]  eta: 0:00:16    time: 8.1273  data: 7.8237  max mem: 14860
Evaluation  [1/2]  eta: 0:00:04    time: 4.3142  data: 4.0274  max mem: 14860
Evaluation Total time: 0:00:08 (4.3152 s / it)
2026-01-24 14:49:28,732 [INFO] Eval Epoch 1, loss: 4.179, waypoints_loss: 3.884, end_loss: 1.476, end_acc: 0.076, 
2026-01-24 14:49:28,734 [INFO] Start training
2026-01-24 14:49:28,750 [INFO] Start training epoch 2, 2 iters per inner epoch.
Train: data epoch: [2]  [0/2]  eta: 0:00:23  lr: 0.000096  loss: 3.8652  waypoints_loss: 3.5829 (3.5829)  end_loss: 1.4115 (1.4115)  end_acc: 0.1831 (0.1831)  time: 11.9264  data: 0.0000  max mem: 14862
Train: data epoch: [2]  [1/2]  eta: 0:00:06  lr: 0.000096  loss: 3.4635  waypoints_loss: 3.1619 (3.3724)  end_loss: 1.4115 (1.4598)  end_acc: 0.1831 (0.1898)  time: 6.3841  data: 0.0000  max mem: 14862
Train: data epoch: [2] Total time: 0:00:12 (6.3847 s / it)
2026-01-24 14:49:41,526 [INFO] Evaluating on val.
Evaluation  [0/2]  eta: 0:00:20    time: 10.0840  data: 9.7722  max mem: 14862
Evaluation  [1/2]  eta: 0:00:05    time: 5.2885  data: 4.9957  max mem: 14862
Evaluation Total time: 0:00:10 (5.2897 s / it)
2026-01-24 14:49:52,996 [INFO] Eval Epoch 2, loss: 4.187, waypoints_loss: 3.902, end_loss: 1.425, end_acc: 0.136, 
2026-01-24 14:49:52,998 [INFO] Start training
2026-01-24 14:49:53,014 [INFO] Start training epoch 3, 2 iters per inner epoch.
Train: data epoch: [3]  [0/2]  eta: 0:00:25  lr: 0.000091  loss: 4.4728  waypoints_loss: 4.2428 (4.2428)  end_loss: 1.1498 (1.1498)  end_acc: 0.3052 (0.3052)  time: 12.6394  data: 0.0000  max mem: 15151
Train: data epoch: [3]  [1/2]  eta: 0:00:06  lr: 0.000091  loss: 2.9940  waypoints_loss: 2.9558 (3.5993)  end_loss: 0.1907 (0.6702)  end_acc: 0.3052 (0.6367)  time: 6.7271  data: 0.0000  max mem: 16434
Train: data epoch: [3] Total time: 0:00:13 (6.7276 s / it)
2026-01-24 14:50:06,474 [INFO] Evaluating on val.
Evaluation  [0/2]  eta: 0:00:16    time: 8.1629  data: 7.8511  max mem: 16434
Evaluation  [1/2]  eta: 0:00:04    time: 4.3295  data: 4.0352  max mem: 16434
Evaluation Total time: 0:00:08 (4.3307 s / it)
2026-01-24 14:50:17,199 [INFO] Eval Epoch 3, loss: 3.428, waypoints_loss: 3.386, end_loss: 0.211, end_acc: 0.957, 
2026-01-24 14:50:17,215 [INFO] Saving checkpoint at epoch 3 to /projappl/project_2014099/lmdrive-fix/LAVIS/lavis/output/drivegpt/cvpr/20260124144748/checkpoint_best.pth.
2026-01-24 14:50:21,449 [INFO] Start training
2026-01-24 14:50:21,464 [INFO] Start training epoch 4, 2 iters per inner epoch.
Train: data epoch: [4]  [0/2]  eta: 0:00:23  lr: 0.000085  loss: 1.6777  waypoints_loss: 1.6396 (1.6396)  end_loss: 0.1904 (0.1904)  end_acc: 0.9710 (0.9710)  time: 11.8843  data: 0.0000  max mem: 16515
Train: data epoch: [4]  [1/2]  eta: 0:00:06  lr: 0.000085  loss: 2.9346  waypoints_loss: 1.6396 (2.2638)  end_loss: 0.1904 (0.2118)  end_acc: 0.9710 (0.9730)  time: 6.3725  data: 0.0000  max mem: 16515
Train: data epoch: [4] Total time: 0:00:12 (6.3731 s / it)
2026-01-24 14:50:34,224 [INFO] Evaluating on val.
Evaluation  [0/2]  eta: 0:00:18    time: 9.0790  data: 8.7775  max mem: 16515
Evaluation  [1/2]  eta: 0:00:04    time: 4.7778  data: 4.4910  max mem: 16515
Evaluation Total time: 0:00:09 (4.7791 s / it)
2026-01-24 14:50:44,927 [INFO] Eval Epoch 4, loss: 2.334, waypoints_loss: 2.281, end_loss: 0.261, end_acc: 0.969, 
2026-01-24 14:50:44,942 [INFO] Saving checkpoint at epoch 4 to /projappl/project_2014099/lmdrive-fix/LAVIS/lavis/output/drivegpt/cvpr/20260124144748/checkpoint_best.pth.
2026-01-24 14:50:49,951 [INFO] Start training
2026-01-24 14:50:49,966 [INFO] Start training epoch 5, 2 iters per inner epoch.
Train: data epoch: [5]  [0/2]  eta: 0:00:26  lr: 0.000077  loss: 2.4744  waypoints_loss: 2.4208 (2.4208)  end_loss: 0.2682 (0.2682)  end_acc: 0.9735 (0.9735)  time: 13.1311  data: 0.0000  max mem: 16687
Train: data epoch: [5]  [1/2]  eta: 0:00:07  lr: 0.000077  loss: 2.2162  waypoints_loss: 2.1737 (2.2972)  end_loss: 0.2125 (0.2404)  end_acc: 0.9735 (0.9743)  time: 7.0114  data: 0.0000  max mem: 16687
Train: data epoch: [5] Total time: 0:00:14 (7.0120 s / it)
2026-01-24 14:51:03,995 [INFO] Evaluating on val.
Evaluation  [0/2]  eta: 0:00:16    time: 8.2762  data: 7.9583  max mem: 16687
Evaluation  [1/2]  eta: 0:00:04    time: 4.4042  data: 4.1009  max mem: 16687
Evaluation Total time: 0:00:08 (4.4055 s / it)
2026-01-24 14:51:14,350 [INFO] Eval Epoch 5, loss: 2.258, waypoints_loss: 2.214, end_loss: 0.221, end_acc: 0.969, 
2026-01-24 14:51:14,365 [INFO] Saving checkpoint at epoch 5 to /projappl/project_2014099/lmdrive-fix/LAVIS/lavis/output/drivegpt/cvpr/20260124144748/checkpoint_best.pth.
2026-01-24 14:51:19,403 [INFO] Start training
2026-01-24 14:51:19,419 [INFO] Start training epoch 6, 2 iters per inner epoch.
Train: data epoch: [6]  [0/2]  eta: 0:00:24  lr: 0.000069  loss: 1.9752  waypoints_loss: 1.9393 (1.9393)  end_loss: 0.1796 (0.1796)  end_acc: 0.9750 (0.9750)  time: 12.0346  data: 0.0000  max mem: 16687
Train: data epoch: [6]  [1/2]  eta: 0:00:06  lr: 0.000069  loss: 1.6108  waypoints_loss: 1.5558 (1.7475)  end_loss: 0.1796 (0.2274)  end_acc: 0.9680 (0.9715)  time: 6.4518  data: 0.0000  max mem: 16687
Train: data epoch: [6] Total time: 0:00:12 (6.4524 s / it)
2026-01-24 14:51:32,330 [INFO] Evaluating on val.
Evaluation  [0/2]  eta: 0:00:19    time: 9.6590  data: 9.3469  max mem: 16687
Evaluation  [1/2]  eta: 0:00:05    time: 5.0837  data: 4.7913  max mem: 16687
Evaluation Total time: 0:00:10 (5.0849 s / it)
2026-01-24 14:51:43,669 [INFO] Eval Epoch 6, loss: 1.863, waypoints_loss: 1.825, end_loss: 0.193, end_acc: 0.969, 
2026-01-24 14:51:43,684 [INFO] Saving checkpoint at epoch 6 to /projappl/project_2014099/lmdrive-fix/LAVIS/lavis/output/drivegpt/cvpr/20260124144748/checkpoint_best.pth.
2026-01-24 14:51:48,733 [INFO] Start training
2026-01-24 14:51:48,749 [INFO] Start training epoch 7, 2 iters per inner epoch.
Train: data epoch: [7]  [0/2]  eta: 0:00:25  lr: 0.000060  loss: 2.0865  waypoints_loss: 2.0503 (2.0503)  end_loss: 0.1813 (0.1813)  end_acc: 0.9722 (0.9722)  time: 12.6172  data: 0.0000  max mem: 16687
Train: data epoch: [7]  [1/2]  eta: 0:00:06  lr: 0.000060  loss: 1.9421  waypoints_loss: 1.9131 (1.9817)  end_loss: 0.1451 (0.1632)  end_acc: 0.9712 (0.9717)  time: 6.7508  data: 0.0000  max mem: 16687
Train: data epoch: [7] Total time: 0:00:13 (6.7514 s / it)
2026-01-24 14:52:02,258 [INFO] Evaluating on val.
Evaluation  [0/2]  eta: 0:00:19    time: 9.8355  data: 9.5324  max mem: 16687
Evaluation  [1/2]  eta: 0:00:05    time: 5.1772  data: 4.8898  max mem: 16687
Evaluation Total time: 0:00:10 (5.1784 s / it)
2026-01-24 14:52:13,748 [INFO] Eval Epoch 7, loss: 1.893, waypoints_loss: 1.854, end_loss: 0.193, end_acc: 0.969, 
2026-01-24 14:52:13,750 [INFO] Start training
2026-01-24 14:52:13,766 [INFO] Start training epoch 8, 2 iters per inner epoch.
Train: data epoch: [8]  [0/2]  eta: 0:00:25  lr: 0.000050  loss: 1.9680  waypoints_loss: 1.9273 (1.9273)  end_loss: 0.2032 (0.2032)  end_acc: 0.9733 (0.9733)  time: 12.6679  data: 0.0000  max mem: 16761
Train: data epoch: [8]  [1/2]  eta: 0:00:06  lr: 0.000050  loss: 2.1268  waypoints_loss: 1.9273 (2.0135)  end_loss: 0.1356 (0.1694)  end_acc: 0.9692 (0.9713)  time: 6.7715  data: 0.0000  max mem: 16761
Train: data epoch: [8] Total time: 0:00:13 (6.7721 s / it)
2026-01-24 14:52:27,316 [INFO] Evaluating on val.
Evaluation  [0/2]  eta: 0:00:19    time: 9.6522  data: 9.3607  max mem: 16761
Evaluation  [1/2]  eta: 0:00:05    time: 5.0820  data: 4.8012  max mem: 16761
Evaluation Total time: 0:00:10 (5.0830 s / it)
2026-01-24 14:52:38,251 [INFO] Eval Epoch 8, loss: 1.745, waypoints_loss: 1.715, end_loss: 0.149, end_acc: 0.970, 
2026-01-24 14:52:38,266 [INFO] Saving checkpoint at epoch 8 to /projappl/project_2014099/lmdrive-fix/LAVIS/lavis/output/drivegpt/cvpr/20260124144748/checkpoint_best.pth.
2026-01-24 14:52:43,306 [INFO] Start training
2026-01-24 14:52:43,321 [INFO] Start training epoch 9, 2 iters per inner epoch.
Train: data epoch: [9]  [0/2]  eta: 0:00:26  lr: 0.000041  loss: 1.9197  waypoints_loss: 1.8960 (1.8960)  end_loss: 0.1182 (0.1182)  end_acc: 0.9748 (0.9748)  time: 13.0219  data: 0.0000  max mem: 16862
Train: data epoch: [9]  [1/2]  eta: 0:00:06  lr: 0.000041  loss: 2.4886  waypoints_loss: 1.8960 (2.1751)  end_loss: 0.1182 (0.1451)  end_acc: 0.9704 (0.9726)  time: 6.9460  data: 0.0000  max mem: 16862
Train: data epoch: [9] Total time: 0:00:13 (6.9467 s / it)
2026-01-24 14:52:57,221 [INFO] Evaluating on val.
Evaluation  [0/2]  eta: 0:00:17    time: 8.5150  data: 8.2009  max mem: 16862
Evaluation  [1/2]  eta: 0:00:04    time: 4.5808  data: 4.2165  max mem: 16862
Evaluation Total time: 0:00:09 (4.5820 s / it)
2026-01-24 14:53:08,731 [INFO] Eval Epoch 9, loss: 1.722, waypoints_loss: 1.687, end_loss: 0.177, end_acc: 0.968, 
2026-01-24 14:53:08,745 [INFO] Saving checkpoint at epoch 9 to /projappl/project_2014099/lmdrive-fix/LAVIS/lavis/output/drivegpt/cvpr/20260124144748/checkpoint_best.pth.
2026-01-24 14:53:13,782 [INFO] Start training
2026-01-24 14:53:13,797 [INFO] Start training epoch 10, 2 iters per inner epoch.
Train: data epoch: [10]  [0/2]  eta: 0:00:26  lr: 0.000033  loss: 1.3175  waypoints_loss: 1.2866 (1.2866)  end_loss: 0.1545 (0.1545)  end_acc: 0.9750 (0.9750)  time: 13.1088  data: 0.0000  max mem: 16862
Train: data epoch: [10]  [1/2]  eta: 0:00:06  lr: 0.000033  loss: 2.1173  waypoints_loss: 1.2866 (1.6880)  end_loss: 0.1396 (0.1470)  end_acc: 0.9750 (0.9750)  time: 6.9953  data: 0.0000  max mem: 16862
Train: data epoch: [10] Total time: 0:00:13 (6.9959 s / it)
2026-01-24 14:53:27,803 [INFO] Evaluating on val.
Evaluation  [0/2]  eta: 0:00:18    time: 9.2216  data: 8.9113  max mem: 16862
Evaluation  [1/2]  eta: 0:00:04    time: 4.9863  data: 4.6954  max mem: 16862
Evaluation Total time: 0:00:09 (4.9874 s / it)
2026-01-24 14:53:37,837 [INFO] Eval Epoch 10, loss: 1.687, waypoints_loss: 1.653, end_loss: 0.167, end_acc: 0.969, 
2026-01-24 14:53:37,851 [INFO] Saving checkpoint at epoch 10 to /projappl/project_2014099/lmdrive-fix/LAVIS/lavis/output/drivegpt/cvpr/20260124144748/checkpoint_best.pth.
2026-01-24 14:53:42,889 [INFO] Start training
2026-01-24 14:53:42,905 [INFO] Start training epoch 11, 2 iters per inner epoch.
Train: data epoch: [11]  [0/2]  eta: 0:00:24  lr: 0.000025  loss: 1.6576  waypoints_loss: 1.6308 (1.6308)  end_loss: 0.1341 (0.1341)  end_acc: 0.9740 (0.9740)  time: 12.2195  data: 0.0000  max mem: 16862
Train: data epoch: [11]  [1/2]  eta: 0:00:06  lr: 0.000025  loss: 1.6463  waypoints_loss: 1.6226 (1.6267)  end_loss: 0.1187 (0.1264)  end_acc: 0.9740 (0.9744)  time: 6.5892  data: 0.0000  max mem: 16862
Train: data epoch: [11] Total time: 0:00:13 (6.5898 s / it)
2026-01-24 14:53:56,091 [INFO] Evaluating on val.
Evaluation  [0/2]  eta: 0:00:20    time: 10.0679  data: 9.7748  max mem: 16862
Evaluation  [1/2]  eta: 0:00:05    time: 5.2686  data: 4.9864  max mem: 16862
Evaluation Total time: 0:00:10 (5.2698 s / it)
2026-01-24 14:54:07,797 [INFO] Eval Epoch 11, loss: 1.671, waypoints_loss: 1.642, end_loss: 0.144, end_acc: 0.969, 
2026-01-24 14:54:07,812 [INFO] Saving checkpoint at epoch 11 to /projappl/project_2014099/lmdrive-fix/LAVIS/lavis/output/drivegpt/cvpr/20260124144748/checkpoint_best.pth.
2026-01-24 14:54:12,834 [INFO] Start training
2026-01-24 14:54:12,850 [INFO] Start training epoch 12, 2 iters per inner epoch.
Train: data epoch: [12]  [0/2]  eta: 0:00:25  lr: 0.000019  loss: 1.7400  waypoints_loss: 1.7038 (1.7038)  end_loss: 0.1808 (0.1808)  end_acc: 0.9646 (0.9646)  time: 12.5227  data: 0.0000  max mem: 16862
Train: data epoch: [12]  [1/2]  eta: 0:00:06  lr: 0.000019  loss: 1.8814  waypoints_loss: 1.7038 (1.7773)  end_loss: 0.1537 (0.1672)  end_acc: 0.9623 (0.9634)  time: 6.6889  data: 0.0000  max mem: 16862
Train: data epoch: [12] Total time: 0:00:13 (6.6895 s / it)
2026-01-24 14:54:26,235 [INFO] Evaluating on val.
Evaluation  [0/2]  eta: 0:00:16    time: 8.4376  data: 8.1352  max mem: 16862
Evaluation  [1/2]  eta: 0:00:04    time: 4.4796  data: 4.1887  max mem: 16862
Evaluation Total time: 0:00:08 (4.4807 s / it)
2026-01-24 14:54:37,835 [INFO] Eval Epoch 12, loss: 1.639, waypoints_loss: 1.609, end_loss: 0.153, end_acc: 0.970, 
2026-01-24 14:54:37,849 [INFO] Saving checkpoint at epoch 12 to /projappl/project_2014099/lmdrive-fix/LAVIS/lavis/output/drivegpt/cvpr/20260124144748/checkpoint_best.pth.
2026-01-24 14:54:42,869 [INFO] Start training
2026-01-24 14:54:42,884 [INFO] Start training epoch 13, 2 iters per inner epoch.
Train: data epoch: [13]  [0/2]  eta: 0:00:26  lr: 0.000014  loss: 2.4010  waypoints_loss: 2.3729 (2.3729)  end_loss: 0.1403 (0.1403)  end_acc: 0.9692 (0.9692)  time: 13.0594  data: 0.0000  max mem: 16862
Train: data epoch: [13]  [1/2]  eta: 0:00:06  lr: 0.000014  loss: 1.7180  waypoints_loss: 1.6973 (2.0351)  end_loss: 0.1032 (0.1218)  end_acc: 0.9692 (0.9720)  time: 6.9631  data: 0.0000  max mem: 16862
Train: data epoch: [13] Total time: 0:00:13 (6.9637 s / it)
2026-01-24 14:54:56,819 [INFO] Evaluating on val.
Evaluation  [0/2]  eta: 0:00:20    time: 10.1817  data: 9.8649  max mem: 16862
Evaluation  [1/2]  eta: 0:00:05    time: 5.3442  data: 5.0494  max mem: 16862
Evaluation Total time: 0:00:10 (5.3454 s / it)
2026-01-24 14:55:08,528 [INFO] Eval Epoch 13, loss: 1.664, waypoints_loss: 1.636, end_loss: 0.138, end_acc: 0.969, 
2026-01-24 14:55:08,530 [INFO] Start training
2026-01-24 14:55:08,546 [INFO] Start training epoch 14, 2 iters per inner epoch.
Train: data epoch: [14]  [0/2]  eta: 0:00:24  lr: 0.000011  loss: 1.6271  waypoints_loss: 1.6070 (1.6070)  end_loss: 0.1006 (0.1006)  end_acc: 0.9750 (0.9750)  time: 12.2543  data: 0.0000  max mem: 16862
Train: data epoch: [14]  [1/2]  eta: 0:00:06  lr: 0.000011  loss: 1.4129  waypoints_loss: 1.3789 (1.4929)  end_loss: 0.1006 (0.1354)  end_acc: 0.9655 (0.9703)  time: 6.5685  data: 0.0000  max mem: 16862
Train: data epoch: [14] Total time: 0:00:13 (6.5691 s / it)
2026-01-24 14:55:21,688 [INFO] Evaluating on val.
Evaluation  [0/2]  eta: 0:00:17    time: 8.7543  data: 8.4440  max mem: 16862
Evaluation  [1/2]  eta: 0:00:04    time: 4.6479  data: 4.3575  max mem: 16862
Evaluation Total time: 0:00:09 (4.6491 s / it)
2026-01-24 14:55:32,857 [INFO] Eval Epoch 14, loss: 1.712, waypoints_loss: 1.682, end_loss: 0.148, end_acc: 0.970, 
2026-01-24 14:55:32,859 [INFO] Training time 0:07:04
