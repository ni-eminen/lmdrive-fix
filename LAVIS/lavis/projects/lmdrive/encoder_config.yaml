encoder_backbone:
  savi:
    resolution: 
      - 192 #THESIS: implemetation, changed from 192 to 400 all occurences in this file
      - 192
    clip_len: 6
    slot_dict:
      num_slots: 14
      slot_size: 256 # changed to 256 from 128 to fit together with qformer assumption
      slot_mlp_size: 256
      num_iterations: 2
    enc_dict:
      num_channels: 3
      enc_channels: [3, 64, 64, 64, 64]
      enc_ks: 5
      enc_out_channels: 256
      enc_norm: ""
    dec_dict:
      dec_channels: [256, 64, 64, 64, 64] # changed to 256 from 128 to fit together with qformer assumption
      dec_resolution: [12, 12]
      dec_ks: [3, 3, 5, 5]
      dec_norm: ""
      upscale: True
    pred_dict:
        pred_type: transformer
        pred_rnn: True
        pred_norm_first: True
        pred_num_layers: 2
        pred_num_heads: 4
        pred_ffn_dim: 512
        pred_sg_every: Null
    loss_dict:
        use_post_recon_loss: True
        recons_loss: mse
        kld_method: none
    test_time_context: 0 # this is 2 in carformer. not sure what this means yet.
    eps: 1e-6
    checkpoint_path_rel: bin/stosavi_64_7slts.pth 
    checkpoint_path: /scratch/project_2014099/previous_runs/town03/lts.pth # Thesis: update this path to latest savi
    legacy: True
    
  pixels_per_meter: 5

  # tokenized_state: ${training.encoder_backbone.tokenized_state}

  bev:
    bevslots: 
      name: bevslots
  bev_crop: center
  bev_crop_size: 400
  bev_size: 400
  object_level: true
  use_slots: true
  tokenized_state: false
  use_past_horizon_states: true
  past_horizon: 2

  encoder_params:
    object_dims: 6
    n_object_classes: 4
    n_embd: 256 # THESIS: what should this be? original 128
    object_level_max_route_length: 2 # THESIS: what should this be?
    object_level_max_num_objects: 0 # THESIS: what should this be?
    object_dropout: 0.25
    obj_mean: [0.6810691, 2.5060852, 4.0731907, 1.7816833, 0.0, 3.1164536]
    obj_std: [15.788632, 10.685628, 1.0298458, 0.4806749, 1.1758355, 1.4236461]

  model_params:
    name: 'ObjectEncoder'
    num_layers: 2
    decoder_layers: 2
